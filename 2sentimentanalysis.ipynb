{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiDDqLxwyCWu",
    "outputId": "379a4446-b116-47f6-fc7f-6af8e734c90c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/vijay_vbr/miniforge3/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /Users/vijay_vbr/miniforge3/lib/python3.9/site-packages (from pyspark) (0.10.9.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/vijay_vbr/miniforge3/lib/python3.9/site-packages (1.23.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BAGai2W9w1dE"
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BAGai2W9w1dE"
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JHbZFSBAxztI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 21:32:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# initializing spark session\n",
    "sc = SparkContext(appName=\"PySparkShell\")\n",
    "# spark = SparkSession(sc)\n",
    "    \n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ooMm5zOpypyC"
   },
   "outputs": [],
   "source": [
    "# define the schema\n",
    "my_schema = tp.StructType([\n",
    "  tp.StructField(name= 'label',       dataType= tp.IntegerType(),  nullable= True),\n",
    "  tp.StructField(name= 'id',          dataType= tp.IntegerType(),  nullable= True),\n",
    "  tp.StructField(name= 'date',          dataType= tp.StringType(),  nullable= True),\n",
    "  tp.StructField(name= 'NO_QUERY',       dataType= tp.StringType(),  nullable= True),\n",
    "  tp.StructField(name= 'username',       dataType= tp.StringType(),  nullable= True),\n",
    "  tp.StructField(name= 'tweet',       dataType= tp.StringType(),   nullable= True)\n",
    "])\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2psU0JNRyr9O"
   },
   "outputs": [],
   "source": [
    " \n",
    "# read the dataset  \n",
    "my_data = spark.read.csv('training.1600000.csv',\n",
    "                         schema=my_schema,\n",
    "                         header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "O_zs8JNY6B68"
   },
   "outputs": [],
   "source": [
    "my_data=my_data.drop(\"id\",\"date\",\"NO_QUERY\",\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = my_data.limit(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data = my_data.sample(n=5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zT6rlwqbywuB",
    "outputId": "55398453-0595-42cc-9b5d-1e364f0b3c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 21:32:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 0, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      " Schema: label, tweet\n",
      "Expected: label but found: 0\n",
      "CSV file: file:///Users/vijay_vbr/Downloads/Real_Time_Data_Analysis-main/training.1600000.csv\n",
      "+-----+--------------------+\n",
      "|label|               tweet|\n",
      "+-----+--------------------+\n",
      "|    0|is upset that he ...|\n",
      "|    0|@Kenichan I dived...|\n",
      "|    0|my whole body fee...|\n",
      "|    0|@nationwideclass ...|\n",
      "|    0|@Kwesidei not the...|\n",
      "|    0|         Need a hug |\n",
      "|    0|@LOLTrish hey  lo...|\n",
      "|    0|@Tatiana_K nope t...|\n",
      "|    0|@twittera que me ...|\n",
      "|    0|spring break in p...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the data\n",
    "my_data.show(10)\n",
    "\n",
    "# print the schema of the file\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SFBxR9ys7XVC"
   },
   "outputs": [],
   "source": [
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "# bag of words count\n",
    "stage_3 = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"cf\", vocabSize=20000, minDF=5)\n",
    "#hashtf = HashingTF(numFeatures=2 ** 16, inputCol=\"wordsWithStopwordsfree\", outputCol=\"tf\")\n",
    "stage_4 = IDF(inputCol=\"cf\", outputCol=\"vector\", minDocFreq=5)\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)\n",
    "# define stage 4: Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "B9pGYIy88WOd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 21:32:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      " Schema: tweet\n",
      "Expected: tweet but found: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "CSV file: file:///Users/vijay_vbr/Downloads/Real_Time_Data_Analysis-main/training.1600000.csv\n",
      "23/05/02 21:32:12 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/02 21:32:12 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/05/02 21:32:12 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/05/02 21:32:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 0, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      " Schema: label, tweet\n",
      "Expected: label but found: 0\n",
      "CSV file: file:///Users/vijay_vbr/Downloads/Real_Time_Data_Analysis-main/training.1600000.csv\n",
      "23/05/02 21:32:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 0, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      " Schema: label, tweet\n",
      "Expected: label but found: 0\n",
      "CSV file: file:///Users/vijay_vbr/Downloads/Real_Time_Data_Analysis-main/training.1600000.csv\n",
      "23/05/02 21:32:12 WARN Instrumentation: [d76e3d9f] All labels are the same value and fitIntercept=true, so the coefficients will be zeros. Training is not needed.\n",
      "23/05/02 21:32:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 0, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      " Schema: label, tweet\n",
      "Expected: label but found: 0\n",
      "CSV file: file:///Users/vijay_vbr/Downloads/Real_Time_Data_Analysis-main/training.1600000.csv\n",
      "23/05/02 21:32:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 0, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      " Schema: label, tweet\n",
      "Expected: label but found: 0\n",
      "CSV file: file:///Users/vijay_vbr/Downloads/Real_Time_Data_Analysis-main/training.1600000.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit.save(\"preprocessingAndLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset  \n",
    "bbc_data = spark.read.csv('BBCNewsTrain.csv',\n",
    "                         header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "G8bFbIprGPNQ"
   },
   "outputs": [],
   "source": [
    "# pipelineFit.save(\"preprocessingAndLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+\n",
      "|ArticleId|                Text|     Category|\n",
      "+---------+--------------------+-------------+\n",
      "|     1833|worldcom ex-boss ...|     business|\n",
      "|      154|german business c...|     business|\n",
      "|     1101|bbc poll indicate...|     business|\n",
      "|     1976|lifestyle  govern...|         tech|\n",
      "|      917|enron bosses in $...|     business|\n",
      "|     1582|howard  truanted ...|     politics|\n",
      "|      651|wales silent on g...|        sport|\n",
      "|     1797|french honour for...|entertainment|\n",
      "|     2034|car giant hit by ...|     business|\n",
      "|     1866|fockers fuel fest...|entertainment|\n",
      "+---------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bbc_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_data=bbc_data.drop('ArticleId')\n",
    "bbc_data=bbc_data.withColumnRenamed(\"Text\",\"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "bbc_data = bbc_data.withColumn(\"Category\", when(bbc_data.Category == \"tech\",0) \\\n",
    "      .when(bbc_data.Category == \"business\",1).when(bbc_data.Category == \"politics\",2).when(bbc_data.Category == \"sport\",3).when(bbc_data.Category == \"entertainment\",4).otherwise(0))\n",
    "\n",
    "#indexers = [StringIndexer(inputCol=\"Category\", outputCol=\"Category_index\")]\n",
    "\n",
    "\n",
    "#pipeline = Pipeline(stages=indexers)\n",
    "#bbc_data = pipeline.fit(bbc_data).transform(bbc_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               tweet|Category|\n",
      "+--------------------+--------+\n",
      "|worldcom ex-boss ...|       1|\n",
      "|german business c...|       1|\n",
      "|bbc poll indicate...|       1|\n",
      "|lifestyle  govern...|       0|\n",
      "|enron bosses in $...|       1|\n",
      "|howard  truanted ...|       2|\n",
      "|wales silent on g...|       3|\n",
      "|french honour for...|       4|\n",
      "|car giant hit by ...|       1|\n",
      "|fockers fuel fest...|       4|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bbc_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "#stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector')\n",
    "\n",
    "# bag of words count\n",
    "stage_3 = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"cf\", vocabSize=20000, minDF=5)\n",
    "#hashtf = HashingTF(numFeatures=2 ** 16, inputCol=\"wordsWithStopwordsfree\", outputCol=\"tf\")\n",
    "stage_4 = IDF(inputCol=\"cf\", outputCol=\"vector\", minDocFreq=5)\n",
    "# define stage 4: Logistic Regression Model\n",
    "model = NaiveBayes(featuresCol= 'vector', labelCol= 'Category',smoothing=1.0, modelType='multinomial')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipelineBBC = Pipeline(stages= [stage_1, stage_2, stage_3,stage_4,model])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF,testDF=bbc_data.randomSplit([0.75, 0.25], seed=2000)\n",
    "# fit the pipeline model with the training data\n",
    "BBCpipelineFit = pipelineBBC.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = BBCpipelineFit.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699242736504335"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "test_df=test_df.withColumnRenamed(\"Category\",\"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBCpipelineFit.save(\"preprocessingAndCategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               tweet|Category|\n",
      "+--------------------+--------+\n",
      "|worldcom ex-boss ...|       1|\n",
      "|german business c...|       1|\n",
      "|bbc poll indicate...|       1|\n",
      "|lifestyle  govern...|       0|\n",
      "|enron bosses in $...|       1|\n",
      "|howard  truanted ...|       2|\n",
      "|wales silent on g...|       3|\n",
      "|french honour for...|       4|\n",
      "|car giant hit by ...|       1|\n",
      "|fockers fuel fest...|       4|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bbc_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentimentanalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
